{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43488ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "\n",
    "FILES_PATH = f\"{os.path.abspath('.')}/data/bronze\"\n",
    "\n",
    "ARREST_PATH = os.path.join(FILES_PATH, \"arrests\")\n",
    "CRIME_PATH = os.path.join(FILES_PATH, \"crimes\")\n",
    "CODE_PATH = os.path.join(FILES_PATH, \"iucr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2975dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/27 20:04:01 WARN Utils: Your hostname, Air-M4.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.65 instead (on interface en0)\n",
      "25/09/27 20:04:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/27 20:04:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"silver-preparation\")\n",
    "    .config(\"spark.jars\", \"./artifacts/postgresql-42.7.7.jar\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1565dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_df = spark.read.options(recursiveFileLookup=\"true\").parquet(CRIME_PATH)\n",
    "arrest_df = spark.read.options(recursiveFileLookup=\"true\").parquet(ARREST_PATH)\n",
    "code_df = spark.read.parquet(CODE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8147727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col, to_timestamp, coalesce, lit, trim, length, when\n",
    "\n",
    "crimes_silver_df = crimes_df.withColumnsRenamed({\n",
    "  \"ID\": \"crime_id\",\n",
    "  \"Case Number\": \"case_number\",\n",
    "  \"Date\": \"crime_datetime\",\n",
    "  \"Block\": \"block\",\n",
    "  \"IUCR\": \"iucr_code\",\n",
    "  \"Primary Type\": \"primary_type\",\n",
    "  \"Description\": \"description\",\n",
    "  \"Location Description\": \"location_description\",\n",
    "  \"Arrest\": \"is_arrest\",\n",
    "  \"Domestic\": \"is_domestic\",\n",
    "  \"Beat\": \"beat_id\",\n",
    "  \"District\": \"district_id\",\n",
    "  \"Ward\": \"ward_id\",\n",
    "  \"Community Area\": \"community_area_id\",\n",
    "  \"FBI Code\": \"fbi_code\",\n",
    "  \"X Coordinate\": \"x_coordinate\",\n",
    "  \"Y Coordinate\": \"y_coordinate\",\n",
    "  \"Latitude\": \"latitude\",\n",
    "  \"Longitude\": \"longitude\",\n",
    "  \"Location\": \"location_geo_point\",\n",
    "}).select(\n",
    "    \"crime_id\", \"case_number\", \"crime_datetime\", \"block\", \"iucr_code\", \"primary_type\",\n",
    "    \"description\", \"location_description\", \"is_arrest\", \"is_domestic\", \"beat_id\", \"ward_id\",\n",
    "    \"community_area_id\", \"fbi_code\", \"x_coordinate\", \"y_coordinate\", \"latitude\", \"longitude\", \n",
    "    \"location_geo_point\", \"district_id\"\n",
    ").withColumns({\n",
    "    \"crime_datetime\": to_timestamp(col(\"crime_datetime\"), \"MM/dd/yyyy hh:mm:ss a\"),\n",
    "    \"is_arrest\": col(\"is_arrest\").cast(BooleanType()),\n",
    "    \"is_domestic\": col(\"is_domestic\").cast(BooleanType()),\n",
    "    \"latitude\": col(\"latitude\").cast(DoubleType()),\n",
    "    \"longitude\": col(\"longitude\").cast(DoubleType()),\n",
    "    \"x_coordinate\": col(\"x_coordinate\").cast(DoubleType()),\n",
    "    \"y_coordinate\": col(\"y_coordinate\").cast(DoubleType()),\n",
    "    \"beat_id\": col(\"beat_id\").cast(IntegerType()),\n",
    "    \"district_id\": col(\"district_id\").cast(IntegerType()),\n",
    "    \"ward_id\": col(\"ward_id\").cast(IntegerType()),\n",
    "    \"community_area_id\": col(\"community_area_id\").cast(IntegerType()),\n",
    "    ## Transform section\n",
    "    \"location_description\": coalesce(col(\"location_description\"), lit(\"Unknown\"))\n",
    "}).filter(\n",
    "    col(\"latitude\").isNotNull() & \\\n",
    "    col(\"longitude\").isNotNull() & \\\n",
    "    col(\"iucr_code\").isNotNull() & \\\n",
    "    col(\"primary_type\").isNotNull()\n",
    ").dropDuplicates([\"crime_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be89ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(name: str) -> str:\n",
    "    name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
    "    name = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name)\n",
    "    return name.replace(\" \", \"_\").lower()\n",
    "\n",
    "arrests_renamed_df = arrest_df.withColumnsRenamed({\n",
    "  # Only this column naming specified, another will be default snake_case\n",
    "  \"CB_NO\": \"arrest_id\",\n",
    "  \"RACE\": \"arrestee_race\",\n",
    "  \"CHARGE 1 STATUTE\": \"charge_one_statue\",\n",
    "  \"CHARGE 1 DESCRIPTION\": \"charge_one_description\",\n",
    "  \"CHARGE 1 TYPE\": \"charge_one_type\",\n",
    "})\n",
    "\n",
    "arrests_silver_df = arrests_renamed_df.toDF(\n",
    "    *[to_snake_case(c) for c in arrests_renamed_df.columns]\n",
    ").select(\n",
    "    \"arrest_id\", \"case_number\", \"arrest_date\", \"arrestee_race\",\n",
    "    \"charge_one_statue\", \"charge_one_description\", \"charge_one_type\"\n",
    ").withColumns({\n",
    "    \"arrest_date\": to_timestamp(col(\"arrest_date\"), \"MM/dd/yyyy hh:mm:ss a\"),\n",
    "    \"charge_one_type\": (\n",
    "        when(col(\"charge_one_type\") == \"F\", \"felony\")\n",
    "        .when(col(\"charge_one_type\") == \"M\", \"misdemeanor\")\n",
    "        .otherwise(\"other\")\n",
    "    )\n",
    "}).filter(\n",
    "    (col(\"case_number\").isNotNull()) & \\\n",
    "    (length(trim(col(\"case_number\"))) > 0)\n",
    ").dropDuplicates([\"arrest_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10efba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+--------------+--------------------+----------------------+---------------+\n",
      "|arrest_id|case_number|        arrest_date| arrestee_race|   charge_one_statue|charge_one_description|charge_one_type|\n",
      "+---------+-----------+-------------------+--------------+--------------------+----------------------+---------------+\n",
      "| 18812380|   HX100068|2014-01-01 01:05:00|WHITE HISPANIC|720 ILCS 5.0/12-3...|  BATTERY - MAKE PH...|    misdemeanor|\n",
      "| 18812401|   HX100135|2014-01-01 01:35:00|         BLACK|720 ILCS 5.0/12-2...|  AGG ASSAULT/POLIC...|    misdemeanor|\n",
      "| 18812508|   HX100292|2014-01-01 04:35:00|         WHITE|720 ILCS 5.0/12-3...|  DOMESTIC BATTERY ...|    misdemeanor|\n",
      "| 18812509|   HX100178|2014-01-01 02:55:00|         BLACK|720 ILCS 5.0/24-1...|  AGG UUW/VEH/FIR L...|         felony|\n",
      "| 18812547|   HX100403|2014-01-01 07:45:00|         BLACK|720 ILCS 5.0/31-4...|  OBSTRUCTING IDENT...|    misdemeanor|\n",
      "+---------+-----------+-------------------+--------------+--------------------+----------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "arrests_silver_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "859734c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_renamed_df = code_df.withColumnsRenamed({\n",
    "    \"IUCR\": \"iucr_code\",\n",
    "    # Another default snake case\n",
    "})\n",
    "\n",
    "codes_silver_df = codes_renamed_df.toDF(\n",
    "    *[to_snake_case(c) for c in codes_renamed_df.columns]\n",
    ").withColumns({\n",
    "    \"primary_description\": coalesce(col(\"primary_description\"), lit(\"Unknown\")),\n",
    "    \"secondary_description\": coalesce(col(\"secondary_description\"), lit(\"Unknown\")),\n",
    "}).filter( # Dont parse all columns to string, because here are boolean\n",
    "    col(\"active\") == True\n",
    ").dropDuplicates([\"iucr_code\"]).select(\n",
    "    \"iucr_code\", \"primary_description\", \"secondary_description\", \"index_code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71fefabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DB_USER = \"airflow\"\n",
    "DB_PASSWORD = \"airflow\"\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:8432/final\"\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def write_psql(\n",
    "        writeable_df: DataFrame,\n",
    "        *,\n",
    "        table: str,\n",
    "        connection_str: str,\n",
    "        username: str,\n",
    "        password: str,\n",
    "        mode: str = \"overwrite\"\n",
    ") -> None:\n",
    "    properties = {\"user\":username, \"password\":password, \"driver\": \"org.postgresql.Driver\"}\n",
    "    writeable_df.write.jdbc(url=connection_str, table=table, properties=properties, mode=mode)\n",
    "\n",
    "write_psql(codes_silver_df, table=\"silver_ucr_codes\", connection_str=JDBC_URL, username=DB_USER, password=DB_PASSWORD)\n",
    "write_psql(arrests_silver_df, table=\"silver_arrests\", connection_str=JDBC_URL, username=DB_USER, password=DB_PASSWORD, mode=\"append\") # pyspark doesnt have mode merge\n",
    "write_psql(crimes_silver_df, table=\"silver_crimes\", connection_str=JDBC_URL, username=DB_USER, password=DB_PASSWORD, mode=\"append\") #  pyspark doesnt have mode merge\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
